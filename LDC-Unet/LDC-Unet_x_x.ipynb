{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Improved LPI Radar Waveform Recognition Framework With LDC-Unet and SSR-Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "This notebook reproduces the improved framework from  \n",
    "**Jiang, Wangkui; Li, Yan; Liao, Mengmeng; and Wang, Shafei (2022)** ‚Äî  \n",
    "*‚ÄúAn Improved LPI Radar Waveform Recognition Framework with LDC-Unet and SSR-Loss.‚Äù*  \n",
    "*IEEE Signal Processing Letters*, **29**, 149‚Äì153. DOI: [10.1109/LSP.2021.3130797](https://doi.org/10.1109/LSP.2021.3130797)  \n",
    "**BibTeX:** [@RN163]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from LDC_Unet import MainModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_algorithm_snr_h5s(root_folder, mod_types):\n",
    "    \"\"\"\n",
    "    Loads .h5 spectrogram files from a specific algorithm's snr_X folder,\n",
    "    filtered by modulation type (FM, PM, HYBRID).\n",
    "\n",
    "    Parameters:\n",
    "    - root_folder (str): Path to the snr_X directory (e.g., .../preprocessed_images/cdae/snr_0)\n",
    "    - mod_types (list): List of modulation categories to include, e.g., ['FM', 'PM']\n",
    "\n",
    "    Returns:\n",
    "    - X: np.ndarray of images\n",
    "    - y: np.ndarray of labels (modulation names as strings)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for mod_type in mod_types:\n",
    "        mod_path = os.path.join(root_folder, mod_type)\n",
    "        if not os.path.exists(mod_path):\n",
    "            print(f\"‚ö†Ô∏è Warning: {mod_path} does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"üìÇ Loading from {mod_type}...\")\n",
    "        files = [f for f in os.listdir(mod_path) if f.endswith(\".h5\")]\n",
    "\n",
    "        for file in tqdm(files, desc=f\"   {mod_type}\", unit=\"file\"):\n",
    "            mod_name = file[:-3]  # Strip '.h5'\n",
    "            file_path = os.path.join(mod_path, file)\n",
    "\n",
    "            try:\n",
    "                with h5py.File(file_path, \"r\") as h5f:\n",
    "                    if mod_name not in h5f:\n",
    "                        print(f\"‚ö†Ô∏è Warning: No top-level group named '{mod_name}' in {file_path}\")\n",
    "                        continue\n",
    "                    group = h5f[mod_name]\n",
    "                    for key in group.keys():\n",
    "                        img = np.array(group[key])\n",
    "                        X.append(img)\n",
    "                        y.append(mod_name)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {file_path}: {e}\")\n",
    "\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader(X, y, batch_size=32, shuffle=False, num_workers=2, device=\"cpu\"):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "    elif not isinstance(X, torch.Tensor):\n",
    "        raise TypeError(\"Input X must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    if isinstance(y, np.ndarray):\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "    elif not isinstance(y, torch.Tensor):\n",
    "        raise TypeError(\"Labels y must be a NumPy array or PyTorch tensor\")\n",
    "\n",
    "    # Ensure X has four dimensions (N, C, H, W)\n",
    "    if X.ndim == 3:  # If (N, H, W), add a channel dimension\n",
    "        X = X.unsqueeze(1)  # (N, 1, H, W)\n",
    "    elif X.ndim == 4 and X.shape[-1] in [1, 3]:  # (N, H, W, C) case\n",
    "        X = X.permute(0, 3, 1, 2)  # Convert to (N, C, H, W)\n",
    "\n",
    "    # Move data to the correct device\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X, y)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=(device == \"cuda\"),\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSRLoss(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim, lambda_reg=0.12):\n",
    "        super(SSRLoss, self).__init__()\n",
    "        self.lambda_reg = lambda_reg  # Weight factor for L1 loss\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, feature_dim))  # Learnable class centers\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, features, labels, logits):\n",
    "        # Compute softmax loss\n",
    "        loss_softmax = self.cross_entropy(logits, labels)\n",
    "\n",
    "        # Get the class centers for each sample\n",
    "        centers_batch = self.centers[labels]  # Shape: (batch_size, feature_dim)\n",
    "\n",
    "        # Compute self-regularization loss (L1 distance)\n",
    "        loss_reg = torch.mean(torch.abs(features - centers_batch))\n",
    "\n",
    "        # Final SSR-Loss\n",
    "        loss = loss_softmax + self.lambda_reg * loss_reg\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    device,\n",
    "    criterion,  # SSRLoss should be passed here\n",
    "    optimizer,\n",
    "    scheduler=None,  # üîß Optional scheduler added\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    min_delta=0.0,\n",
    "):\n",
    "\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_history = []\n",
    "    best_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Progress bar for visualization\n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            desc=f\"Epoch {epoch+1}/{epochs}\",\n",
    "            leave=True,\n",
    "            dynamic_ncols=True,\n",
    "        )\n",
    "\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass - model should return (features, logits)\n",
    "            features, logits = model(inputs)\n",
    "\n",
    "            # Compute SSR-Loss using features and logits\n",
    "            loss = criterion(features, labels, logits)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Live loss display\n",
    "            progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        # Average loss for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")\n",
    "\n",
    "        model_file_name = f\"model_latest\"\n",
    "        if epoch % 5:\n",
    "            torch.save(model.state_dict(), model_file_name + \".pth\")\n",
    "\n",
    "        # üîÑ Scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss - min_delta:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return loss_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1 Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(loss_history, output_path, title=\"Training Loss Over Epochs\"):\n",
    "    epochs = len(loss_history)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, epochs + 1), loss_history, marker=\"o\", label=\"Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(output_path + f\"loss_curve.png\")\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.2 Conf Matirx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(\n",
    "    model, data_loader, device, output_path, class_names=None, title=\"Confusion Matrix\"\n",
    "):\n",
    "    # Switch model to evaluation mode\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Disable gradient calculations for inference\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass: Ignore output_image, focus only on output_class\n",
    "            _, output_class = model(inputs)\n",
    "\n",
    "            # Get predicted class labels\n",
    "            preds = torch.argmax(output_class, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    num_classes = cm.shape[0]\n",
    "    \n",
    "    # Normalize confusion matrix to percentages\n",
    "    cm_normalized = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    # If class_names isn't provided, use numeric class indices\n",
    "    if class_names is None:\n",
    "        class_names = [str(i) for i in range(num_classes)]\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(max(10, num_classes * 0.8), max(8, num_classes * 0.6)))  # Dynamic size\n",
    "    im = plt.imshow(cm_normalized, interpolation=\"nearest\", cmap=\"Blues\")\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar(im, label=\"Percentage\")  # Add colorbar with label\n",
    "\n",
    "    # Create tick marks for class labels\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, class_names, rotation=45, ha=\"right\", va=\"top\", fontsize=max(8, 12 - num_classes // 5))\n",
    "    plt.yticks(tick_marks, class_names, fontsize=max(8, 12 - num_classes // 5))\n",
    "\n",
    "    # Annotate the matrix cells with percentage values\n",
    "    thresh = cm_normalized.max() / 2.0\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            plt.text(\n",
    "                j,\n",
    "                i,\n",
    "                f\"{cm_normalized[i, j]:.1f}\",\n",
    "                ha=\"center\",\n",
    "                va=\"center\",\n",
    "                color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n",
    "                fontsize=max(8, 12 - num_classes // 5),\n",
    "            )\n",
    "\n",
    "    plt.ylabel(\"True Label\", fontsize=12, labelpad=10)\n",
    "    plt.xlabel(\"Predicted Label\", fontsize=12, labelpad=10)\n",
    "    \n",
    "    # Adjust layout with extra bottom margin for rotated labels\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2 + num_classes * 0.005)  # Dynamic bottom margin\n",
    "    \n",
    "    plt.savefig(output_path + f\"conf_matrix.png\")\n",
    "\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, label_encoder, device, output_path):\n",
    "\n",
    "    model.to(device)  # Ensure model is on correct device\n",
    "    model.eval()  # Set to evaluation mode\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Model returns (output_image, output_class)\n",
    "            _, output_class = model(inputs)\n",
    "\n",
    "            # Get predicted class (argmax over logits)\n",
    "            preds = torch.argmax(output_class, dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            y_true.extend(labels.cpu().tolist())  # Move to CPU for metrics\n",
    "            y_pred.extend(preds.cpu().tolist())\n",
    "\n",
    "    # Compute Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # Compute & Display Confusion Matrix\n",
    "    class_names = label_encoder.classes_  # Decode label names\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Normalize confusion matrix to percentages\n",
    "    cm_normalized = cm.astype(np.float32) / cm.sum(axis=1, keepdims=True) * 100\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(max(10, num_classes * 0.8), max(8, num_classes * 0.6)))  # Dynamic size\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".1f\", ax=ax)  # Use 1 decimal place for percentages\n",
    "\n",
    "    # Adjust x-axis label alignment and font sizes\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha=\"right\", va=\"top\", fontsize=max(8, 12 - num_classes // 5))\n",
    "    ax.set_yticklabels(class_names, rotation=0, fontsize=max(8, 12 - num_classes // 5))\n",
    "    ax.set_xlabel(\"Predicted Label\", fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel(\"True Label\", fontsize=12, labelpad=10)\n",
    "    ax.set_title(\"Confusion Matrix (Percentage)\", fontsize=14)\n",
    "\n",
    "    # Adjust layout with extra bottom margin for rotated labels\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.2 + num_classes * 0.005)  # Dynamic bottom margin\n",
    "\n",
    "    plt.savefig(output_path + \"test_conf_matrix.png\")\n",
    "\n",
    "    plt.clf()\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_in_one_training_testing(data_path, snr, mod_types, input_parameters, output_path):\n",
    "\n",
    "    input_data_folder = data_path + f\"snr_{snr}\"\n",
    "    os.makedirs(input_data_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "    epoch_count = input_parameters[\"epoch_count\"]\n",
    "    learning_rate = input_parameters[\"learning_rate\"]\n",
    "\n",
    "    mds = mod_types[0]\n",
    "    if len(mod_types) == 3:\n",
    "        mds = \"ALL\"\n",
    "\n",
    "    output_data_folder = output_path + f\"snr_{snr}_mds_{mds}_e{epoch_count}_lr{learning_rate}\\\\\"\n",
    "    os.makedirs(output_data_folder, exist_ok=True)\n",
    "\n",
    "    print(f\"Loading {input_data_folder}\")\n",
    "\n",
    "    X, y = load_algorithm_snr_h5s(input_data_folder, mod_types)\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    joblib.dump(label_encoder, output_data_folder + f\"label_encoder.pkl\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "\n",
    "    # Prepare DataLoaders\n",
    "    train_loader = prepare_dataloader(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    model = MainModel(num_classes=len(np.unique(y_encoded))).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = SSRLoss(num_classes=len(np.unique(y_encoded)), feature_dim=256, lambda_reg=0.12).to(device)    \n",
    "        \n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=45, gamma=0.1)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Train the model\n",
    "    loss_history = train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        epochs=epoch_count,\n",
    "        patience=50,\n",
    "    )\n",
    "\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    # Save loss_history as .csv\n",
    "    np.savetxt(output_data_folder + f\"loss_history.csv\", loss_history, delimiter=\",\")\n",
    "\n",
    "    plot_loss_curve(loss_history, output_data_folder)\n",
    "\n",
    "    display_confusion_matrix(model, train_loader, device, output_data_folder)\n",
    "\n",
    "    model_file_name = f\"model_snr_{snr}_mds_{mds}_e{epoch_count}_lr{learning_rate}\"\n",
    "\n",
    "    model_file_name = f\"model_snr_{snr}_mds_{mds}_e{epoch_count}_lr{learning_rate}.pth\"\n",
    "    model_file_path = os.path.join(output_data_folder, model_file_name)\n",
    "\n",
    "    torch.save(model, model_file_path)\n",
    "\n",
    "    # Prepare DataLoaders\n",
    "    test_loader = prepare_dataloader(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    acc = evaluate_model(model, test_loader, label_encoder, device, output_data_folder)\n",
    "\n",
    "    df = pd.read_csv(output_path + f\"{input_parameters[\"csv\"]}_results.csv\")\n",
    "\n",
    "    new_row = {\n",
    "        \"Algorithm\": \"LDC-Unet\",\n",
    "        \"SNR\": snr,\n",
    "        \"Modulations\": mds,\n",
    "        \"Accuracy (%)\": acc,\n",
    "        \"Time Taken (Minutes)\": time_taken,\n",
    "        \"Learning Rate\": learning_rate,\n",
    "        \"Epoch Count\": f\"{len(loss_history)} / {epoch_count}\"\n",
    "    }\n",
    "\n",
    "    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    df.to_csv(output_path + f\"{input_parameters[\"csv\"]}_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the main directory exists\n",
    "data_path = \"C:\\\\Apps\\\\Code\\\\aimc-spec-7\\\\preprocessed_images\\\\ldc\\\\\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Ensure the main directory exists\n",
    "output_path = \"C:\\\\Apps\\\\Code\\\\LDC_Unet\\\\\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "snr_range = [10, 5, 0, -2, -4, -6, -8, -10, -12, -14, -16, -18, -20]\n",
    "\n",
    "modulation_types = [\n",
    "    \"FM\",\n",
    "    # \"PM\",\n",
    "    # \"HYBRID\",\n",
    "]\n",
    "\n",
    "input_parameters = {\n",
    "    \"epoch_count\": 200,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"csv\": \"ldc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for snr in snr_range:\n",
    "    all_in_one_training_testing(data_path, snr, modulation_types, input_parameters, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
